{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Comparing ResNet scratch vs TransferLearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMov-LUSabxx"
      },
      "source": [
        "# Comparing ResNet scratch vs TransferLearning\n",
        "@hyyoka\n",
        "\n",
        "><img src=\"https://drive.google.com/uc?id=1YQkxnNy61Gyi3Gp6ylCKeS72BVruJXr_\" width=\"700\" height=\"500\"> \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7LJdJfU6pra",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca49e46-7324-4916-d33f-f6b78106103a"
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.7/dist-packages (0.4.4)\n",
            "Requirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FlbGAEe6prv"
      },
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "# import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YodbjXPi6psK"
      },
      "source": [
        "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Accuracy, Loss\n",
        "from ignite.handlers import EarlyStopping, ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLNq8A8hii8K"
      },
      "source": [
        "# Data 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ-2mRGqgCp3",
        "outputId": "0450be8f-3f5f-4771-d6de-8a9fdc5913d5"
      },
      "source": [
        "# gdrive에 mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "# 경로 설정\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmRGSpMXgI_W"
      },
      "source": [
        "# import shutil\n",
        " \n",
        "# original_dataset_dir = '/content/gdrive/MyDrive/tobigs_week9_plant_leaf'   \n",
        "# classes_list = os.listdir(original_dataset_dir) \n",
        " \n",
        "# base_dir = './splitted' \n",
        "# os.mkdir(base_dir)\n",
        " \n",
        "# train_dir = os.path.join(base_dir, 'train') \n",
        "# os.mkdir(train_dir)\n",
        "# validation_dir = os.path.join(base_dir, 'val')\n",
        "# os.mkdir(validation_dir)\n",
        "# test_dir = os.path.join(base_dir, 'test')\n",
        "# os.mkdir(test_dir)\n",
        "\n",
        "# for cls in classes_list:     \n",
        "#     os.mkdir(os.path.join(train_dir, cls))\n",
        "#     os.mkdir(os.path.join(validation_dir, cls))\n",
        "#     os.mkdir(os.path.join(test_dir, cls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIdNGq_YCtsm"
      },
      "source": [
        "# print(classes_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3d6m17egXa2"
      },
      "source": [
        "# import math\n",
        " \n",
        "# for cls in classes_list[1:]:\n",
        "#     path = os.path.join(original_dataset_dir, cls)\n",
        "#     fnames = os.listdir(path)\n",
        " \n",
        "#     train_size = math.floor(len(fnames) * 0.6)\n",
        "#     validation_size = math.floor(len(fnames) * 0.2)\n",
        "#     test_size = math.floor(len(fnames) * 0.2)\n",
        "    \n",
        "#     train_fnames = fnames[:train_size]\n",
        "#     print(\"Train size(\",cls,\"): \", len(train_fnames))\n",
        "#     for fname in train_fnames:\n",
        "#         src = os.path.join(path, fname)\n",
        "#         dst = os.path.join(os.path.join(train_dir, cls), fname)\n",
        "#         shutil.copyfile(src, dst)\n",
        "        \n",
        "#     validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
        "#     print(\"Validation size(\",cls,\"): \", len(validation_fnames))\n",
        "#     for fname in validation_fnames:\n",
        "#         src = os.path.join(path, fname)\n",
        "#         dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
        "#         shutil.copyfile(src, dst)\n",
        "        \n",
        "#     test_fnames = fnames[(train_size+validation_size):(validation_size + train_size +test_size)]\n",
        "\n",
        "#     print(\"Test size(\",cls,\"): \", len(test_fnames))\n",
        "#     for fname in test_fnames:\n",
        "#         src = os.path.join(path, fname)\n",
        "#         dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
        "#         shutil.copyfile(src, dst)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2dqyxnr6psU"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "BATCH_SIZE = 256 \n",
        "EPOCH = 30 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgL7kOiQixrw"
      },
      "source": [
        "transform_base = transforms.Compose([transforms.Resize((64,64)),transforms.ToTensor()]) \n",
        "train_dataset = ImageFolder(root='./splitted/train', transform=transform_base) \n",
        "val_dataset = ImageFolder(root='./splitted/val', transform=transform_base)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QTBq9E6iUm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "935eb45a-0ca2-4a44-e22b-1ff144d9f64e"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset,batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvzm-Oe1abyB"
      },
      "source": [
        "# Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JzKXHrlP2T9"
      },
      "source": [
        "classes_list = ['Apple___healthy', 'Apple___Black_rot', 'Apple___Apple_scab', 'Pepper,_bell___Bacterial_spot', 'Corn___healthy', 'Corn___Northern_Leaf_Blight', 'Potato___healthy', 'Corn___Common_rust', 'Cherry___healthy', 'Strawberry___Leaf_scorch', 'Pepper,_bell___healthy', 'Grape___Black_rot', 'Grape___Esca_(Black_Measles)', 'Potato___Early_blight', 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)', 'Grape___healthy', 'Potato___Late_blight', 'Corn___Cercospora_leaf_spot Gray_leaf_spot', 'Cherry___Powdery_mildew', 'Apple___Cedar_apple_rust', 'Strawberry___healthy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8uMCEbsQGHu",
        "outputId": "b9c88ea1-98e9-4810-c854-4b1f2ad73f48"
      },
      "source": [
        "num_classes = len(classes_list)\n",
        "num_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K5_uCVzk-lR"
      },
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self,in_ch1,out_ch1, stride1, in_ch2,out_ch2, stride2):\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.BatchNorm2d(num_features=in_ch1),\n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.Conv2d(in_channels=in_ch1, out_channels=out_ch1, kernel_size=3, stride=stride1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=in_ch2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=in_ch2, out_channels=out_ch2, kernel_size=3, stride=stride2, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return x + out\n",
        "\n",
        "class ResBlock2(nn.Module):\n",
        "    def __init__(self,in_ch1,out_ch1, stride1, in_ch2,out_ch2, stride2):\n",
        "        super(ResBlock2, self).__init__()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.BatchNorm2d(num_features=in_ch1),\n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.Conv2d(in_channels=in_ch1, out_channels=out_ch1, kernel_size=3, stride=stride1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(num_features=in_ch2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels=in_ch2, out_channels=out_ch2, kernel_size=3, stride=stride2, padding=1, bias=False)\n",
        "        )\n",
        "        \n",
        "        self.revise = nn.Sequential(\n",
        "            nn.BatchNorm2d(num_features=in_ch1),\n",
        "            nn.ReLU(inplace=True), \n",
        "            nn.Conv2d(in_ch1, out_ch1, 1,2)\n",
        "        ) \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        x = self.revise(x)\n",
        "        return x + out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgW03WIDlCCO"
      },
      "source": [
        "class IdentityResNet(nn.Module):\n",
        "    # nblk_stage1: number of blocks in stage 1, nblk_stage2.. \n",
        "    def __init__(self, nblk_stage1, nblk_stage2, nblk_stage3, nblk_stage4):\n",
        "        super(IdentityResNet, self).__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.stage1 = nn.Sequential(\n",
        "            ResBlock(64, 64, 1, 64, 64, 1),\n",
        "            ResBlock(64, 64, 1, 64, 64, 1)\n",
        "        )\n",
        "        self.stage2 = nn.Sequential(\n",
        "            ResBlock2(64, 128, 2, 128, 128, 1),\n",
        "            ResBlock(128, 128, 1, 128, 128, 1)\n",
        "        )\n",
        "        self.stage3 = nn.Sequential(\n",
        "            ResBlock2(128, 256, 2, 256, 256, 1),\n",
        "            ResBlock(256, 256, 1, 256, 256, 1)\n",
        "        )\n",
        "        self.stage4 = nn.Sequential(\n",
        "            ResBlock2(256, 512, 2, 512, 512, 1),\n",
        "            ResBlock(512, 512, 1, 512, 512, 1)\n",
        "        )\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size = 4, stride=6)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        conv = self.conv(x)\n",
        "        sg1 = self.stage1(conv)\n",
        "        sg2 = self.stage2(sg1)\n",
        "        sg3 = self.stage3(sg2)\n",
        "        sg4 = self.stage4(sg3)\n",
        "        avg = self.avgpool(sg4)\n",
        "        print(avg.shape)\n",
        "        avg = avg.reshape(BATCH_SIZE, 512)\n",
        "        print(avg.shape)\n",
        "        out = self.fc(avg)\n",
        "        print(out.shape)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXyVBBYw6pst"
      },
      "source": [
        "model = IdentityResNet(nblk_stage1=2, nblk_stage2=2,\n",
        "                     nblk_stage3=2, nblk_stage4=2)\n",
        "model.to(DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
        "criterion = nn.CrossEntropyLoss() # nn.LogSoftmax + nn.NLLLoss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o69S4bEvabyI"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7EaSDMW6ps5"
      },
      "source": [
        "# trainer\n",
        "trainer = create_supervised_trainer(model, optimizer, criterion, device=DEVICE)\n",
        "\n",
        "# evaluator\n",
        "metrics = {'accuracy':Accuracy(),'loss':Loss(criterion)}\n",
        "train_evaluator = create_supervised_evaluator(model, metrics=metrics, device=DEVICE)\n",
        "train_history = {'accuracy':[],'loss':[]}\n",
        "\n",
        "val_evaluator = create_supervised_evaluator(model, metrics=metrics, device=DEVICE)\n",
        "val_history = {'accuracy':[],'loss':[]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFQNaZDx6ptV"
      },
      "source": [
        "@trainer.on(Events.EPOCH_COMPLETED) # 한 에폭이 끝날 때마다 실행하게끔 trainer에 추가\n",
        "def training_log(trainer):\n",
        "    train_evaluator.run(train_loader)\n",
        "    metrics = train_evaluator.state.metrics\n",
        "    accuracy = metrics['accuracy']*100\n",
        "    loss = metrics['loss']\n",
        "    train_history['accuracy'].append(accuracy)\n",
        "    train_history['loss'].append(loss)\n",
        "    print(\"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\".format(trainer.state.epoch, accuracy, loss))\n",
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def validation_log(trainer):\n",
        "    val_evaluator.run(val_loader)\n",
        "    metrics = val_evaluator.state.metrics\n",
        "    accuracy = metrics['accuracy']*100\n",
        "    loss = metrics['loss']\n",
        "    val_history['accuracy'].append(accuracy)\n",
        "    val_history['loss'].append(loss)\n",
        "    print(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\".format(trainer.state.epoch, accuracy, loss))\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ztJE13LyfJF"
      },
      "source": [
        "### Training!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hB8goRBQznuM",
        "outputId": "7628dbde-6549-459b-96c5-aee5c21f2035"
      },
      "source": [
        "checkpointer = ModelCheckpoint('/content/', 'TOMATO', n_saved=2, create_dir=True, save_as_state_dict=True, require_empty=False)\n",
        "trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'TOMATO': model})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ignite.engine.events.RemovableEventHandle at 0x7ff062223690>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW6m6Stp6pty"
      },
      "source": [
        "trainer.run(train_loader, max_epochs=EPOCH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teQNkIi9z9jS"
      },
      "source": [
        "# loading the saved model\n",
        "def fetch_last_checkpoint_model_filename(model_save_path):\n",
        "    import os\n",
        "    checkpoint_files = os.listdir(model_save_path)\n",
        "    checkpoint_files = [f for f in checkpoint_files if '.pt' in f]\n",
        "    checkpoint_iter = [\n",
        "        int(x.split('_')[2].split('.')[0])\n",
        "        for x in checkpoint_files]\n",
        "    last_idx = np.array(checkpoint_iter).argmax()\n",
        "    return os.path.join(model_save_path, checkpoint_files[last_idx])\n",
        "\n",
        "model.load_state_dict(torch.load(fetch_last_checkpoint_model_filename('/content')))\n",
        "print(\"Model Loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTwbdtI_abyg"
      },
      "source": [
        "### Accuracy plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SptDgBR66pt4"
      },
      "source": [
        "plt.plot(train_history['accuracy'],label=\"Training Accuracy\")\n",
        "plt.plot(val_history['accuracy'],label=\"Validation Accuracy\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrUnOEIfzLid"
      },
      "source": [
        "plt.plot(train_history['loss'],label=\"Training loss\")\n",
        "plt.plot(val_history['loss'],label=\"Validation loss\")\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io9erWvXjyFO"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqosAimhkCk5"
      },
      "source": [
        "from torchvision import models\n",
        " \n",
        "resnet = models.resnet50(pretrained=True)  \n",
        "num_ftrs = resnet.fc.in_features   \n",
        "resnet.fc = nn.Linear(num_ftrs, num_classes \n",
        "resnet = resnet.to(DEVICE)\n",
        " \n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQmjnc62kCiH"
      },
      "source": [
        "# Pre-Trained Model의 일부 Layer Freeze하기 (resnet 기준입니다 !!)\n",
        "ct = 0 \n",
        "for child in resnet.children():  \n",
        "    ct+= 1  \n",
        "    if ct < 6: \n",
        "        for param in child.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-Tb0Na9qDK9"
      },
      "source": [
        "# trainer\n",
        "f_trainer = create_supervised_trainer(resnet, optimizer_ft, criterion, device=DEVICE)\n",
        "\n",
        "# evaluator\n",
        "metrics = {'accuracy':Accuracy(),'loss':Loss(criterion)}\n",
        "train_evaluator = create_supervised_evaluator(resnet, metrics=metrics, device=DEVICE)\n",
        "train_history = {'accuracy':[],'loss':[]}\n",
        "\n",
        "val_evaluator = create_supervised_evaluator(resnet, metrics=metrics, device=DEVICE)\n",
        "val_history = {'accuracy':[],'loss':[]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY_ZGTHhkCfN"
      },
      "source": [
        "f_trainer.run(train_loader, max_epochs=EPOCH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_Y4K0W6kCZd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}